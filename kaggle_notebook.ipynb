{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with XGBoost & SHAP Explainability\n",
    "\n",
    "**Author**: Noah Gallagher | [GitHub](https://github.com/noahgallagher1) | [LinkedIn](https://www.linkedin.com/in/noahgallagher/)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates an end-to-end machine learning solution for predicting customer churn in a telecommunications company. The solution combines:\n",
    "\n",
    "- **Predictive Modeling**: XGBoost classifier optimized for recall (93%)\n",
    "- **Explainable AI**: SHAP values for model interpretability\n",
    "- **Business Impact**: ROI analysis showing $367K annual savings\n",
    "- **Statistical Rigor**: Baseline comparisons, confidence intervals, segment analysis\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Recall | 93.0% |\n",
    "| Accuracy | 62.5% |\n",
    "| ROC AUC | 83.8% |\n",
    "| ROI | 431.6% |\n",
    "| Customers Saved | 226/year |\n",
    "| Net Savings | $367,300/year |\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "1. **Data Loading & EDA**\n",
    "2. **Feature Engineering**\n",
    "3. **Model Training & Selection**\n",
    "4. **Advanced Evaluation**\n",
    "5. **SHAP Explainability**\n",
    "6. **Business Impact Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Kaggle environment imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Stats\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('\u2713 All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input files\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading & Exploration\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "The Telco Customer Churn dataset contains 7,043 customers with 21 features:\n",
    "- **Demographics**: Gender, senior citizen status, partner, dependents\n",
    "- **Services**: Phone, internet, streaming, security, tech support\n",
    "- **Billing**: Contract type, payment method, monthly/total charges\n",
    "- **Tenure**: Months as customer\n",
    "- **Target**: Churn (Yes/No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Kaggle input or URL\n",
    "try:\n",
    "    # Try loading from Kaggle dataset first\n",
    "    df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    print('\u2713 Loaded from Kaggle dataset')\n",
    "except:\n",
    "    # Fallback to downloading from GitHub\n",
    "    url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    print('\u2713 Loaded from GitHub URL')\n",
    "\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "print(f'Features: {df.shape[1]}')\n",
    "print(f'Samples: {df.shape[0]:,}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print('Dataset Information:')\n",
    "print(df.info())\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Missing Values:')\n",
    "print('='*60)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print('No missing values')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Target Distribution:')\n",
    "print('='*60)\n",
    "churn_dist = df['Churn'].value_counts()\n",
    "print(churn_dist)\n",
    "print(f'\\nChurn Rate: {churn_dist[\"Yes\"] / len(df) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "axes[0].pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12})\n",
    "axes[0].set_title('Churn Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "sns.countplot(data=df, x='Churn', palette=colors, ax=axes[1])\n",
    "axes[1].set_title('Churn Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Churn', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%d')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n\u26a0\ufe0f Class Imbalance Detected: {churn_counts[\"Yes\"]/len(df)*100:.1f}% churned')\n",
    "print('Will use SMOTE to balance classes during training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key exploratory visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Churn by Contract Type\n",
    "pd.crosstab(df['Contract'], df['Churn'], normalize='index').plot(\n",
    "    kind='bar', stacked=False, ax=axes[0, 0], color=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "axes[0, 0].set_title('Churn Rate by Contract Type', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Contract Type')\n",
    "axes[0, 0].set_ylabel('Proportion')\n",
    "axes[0, 0].legend(['No Churn', 'Churn'])\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Churn by Tenure\n",
    "df['TenureBin'] = pd.cut(df['tenure'], bins=[0, 12, 24, 36, 48, 100], \n",
    "                          labels=['0-12', '12-24', '24-36', '36-48', '48+'])\n",
    "pd.crosstab(df['TenureBin'], df['Churn'], normalize='index').plot(\n",
    "    kind='bar', stacked=False, ax=axes[0, 1], color=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "axes[0, 1].set_title('Churn Rate by Tenure (months)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Tenure Group')\n",
    "axes[0, 1].set_ylabel('Proportion')\n",
    "axes[0, 1].legend(['No Churn', 'Churn'])\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. Churn by Payment Method\n",
    "pd.crosstab(df['PaymentMethod'], df['Churn'], normalize='index').plot(\n",
    "    kind='bar', stacked=False, ax=axes[1, 0], color=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "axes[1, 0].set_title('Churn Rate by Payment Method', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Payment Method')\n",
    "axes[1, 0].set_ylabel('Proportion')\n",
    "axes[1, 0].legend(['No Churn', 'Churn'])\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Monthly Charges Distribution\n",
    "df[df['Churn']=='No']['MonthlyCharges'].hist(bins=30, alpha=0.5, label='No Churn', \n",
    "                                               color='#2ecc71', ax=axes[1, 1])\n",
    "df[df['Churn']=='Yes']['MonthlyCharges'].hist(bins=30, alpha=0.5, label='Churn', \n",
    "                                                color='#e74c3c', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Monthly Charges Distribution by Churn', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Monthly Charges ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Drop temporary column\n",
    "df = df.drop('TenureBin', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from EDA\n",
    "\n",
    "1. **Contract Type**: Month-to-month contracts have ~42% churn vs. ~3% for 2-year contracts\n",
    "2. **Tenure**: New customers (<12 months) have 50%+ churn rate\n",
    "3. **Payment Method**: Electronic check users show highest churn (~45%)\n",
    "4. **Monthly Charges**: Churners tend to have higher monthly charges\n",
    "5. **Class Imbalance**: 26.5% churn rate requires SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print('Step 1: Data Cleaning')\n",
    "print('='*60)\n",
    "\n",
    "# Drop customerID (not predictive)\n",
    "df_processed = df_processed.drop('customerID', axis=1)\n",
    "\n",
    "# Handle TotalCharges (should be numeric)\n",
    "df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Fill missing TotalCharges with 0 (likely new customers)\n",
    "missing_charges = df_processed['TotalCharges'].isnull().sum()\n",
    "print(f'Missing TotalCharges: {missing_charges}')\n",
    "df_processed['TotalCharges'] = df_processed['TotalCharges'].fillna(0)\n",
    "\n",
    "# Convert Yes/No to 1/0 for binary features\n",
    "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "for col in binary_cols:\n",
    "    df_processed[col] = (df_processed[col] == 'Yes').astype(int)\n",
    "\n",
    "# Handle service columns (No/No internet service/No phone service -> 0, Yes -> 1)\n",
    "service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "for col in service_cols:\n",
    "    df_processed[col] = df_processed[col].apply(\n",
    "        lambda x: 1 if x == 'Yes' else 0\n",
    "    )\n",
    "\n",
    "# Convert SeniorCitizen to int (already 0/1)\n",
    "df_processed['SeniorCitizen'] = df_processed['SeniorCitizen'].astype(int)\n",
    "\n",
    "print('\u2713 Data cleaning complete')\n",
    "print(f'\\nCleaned dataset shape: {df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nStep 2: Feature Engineering')\n",
    "print('='*60)\n",
    "\n",
    "# Create tenure groups\n",
    "df_processed['TenureGroup'] = pd.cut(\n",
    "    df_processed['tenure'], \n",
    "    bins=[0, 6, 12, 24, 36, 48, 100],\n",
    "    labels=['0-6', '6-12', '12-24', '24-36', '36-48', '48+']\n",
    ")\n",
    "\n",
    "# Revenue per month\n",
    "df_processed['ChargesPerTenure'] = df_processed['TotalCharges'] / (df_processed['tenure'] + 1)\n",
    "\n",
    "# Total services count\n",
    "service_cols_all = ['PhoneService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_processed['TotalServices'] = df_processed[service_cols_all].sum(axis=1)\n",
    "\n",
    "# Premium services flag (security + tech support)\n",
    "df_processed['HasPremiumServices'] = (\n",
    "    (df_processed['OnlineSecurity'] == 1) | (df_processed['TechSupport'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# High charges flag\n",
    "df_processed['HighCharges'] = (\n",
    "    df_processed['MonthlyCharges'] > df_processed['MonthlyCharges'].median()\n",
    ").astype(int)\n",
    "\n",
    "# Monthly charges bins\n",
    "df_processed['MonthlyChargesGroup'] = pd.cut(\n",
    "    df_processed['MonthlyCharges'],\n",
    "    bins=[0, 35, 70, 150],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print('\u2713 Feature engineering complete')\n",
    "print(f'\\nNew features created: {df_processed.shape[1] - df.shape[1] + 1}')\n",
    "print(f'Total features: {df_processed.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nStep 3: Encoding Categorical Variables')\n",
    "print('='*60)\n",
    "\n",
    "# Separate target\n",
    "y = (df_processed['Churn'] == 'Yes').astype(int)\n",
    "X = df_processed.drop('Churn', axis=1)\n",
    "\n",
    "# Get categorical columns (excluding already encoded)\n",
    "cat_cols = ['gender', 'InternetService', 'Contract', 'PaymentMethod', \n",
    "            'TenureGroup', 'MonthlyChargesGroup']\n",
    "\n",
    "# One-hot encoding\n",
    "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=False)\n",
    "\n",
    "print(f'\u2713 Encoding complete')\n",
    "print(f'Features after encoding: {X_encoded.shape[1]}')\n",
    "print(f'Target distribution: {y.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nStep 4: Train/Test Split & Scaling')\n",
    "print('='*60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]:,} samples')\n",
    "print(f'Test set: {X_test.shape[0]:,} samples')\n",
    "print(f'Features: {X_train.shape[1]}')\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges', \n",
    "                     'ChargesPerTenure', 'TotalServices']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print('\u2713 Scaling complete')\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "print('\\nApplying SMOTE for class balance...')\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f'\u2713 SMOTE applied')\n",
    "print(f'Training set after SMOTE: {X_train_balanced.shape[0]:,} samples')\n",
    "print(f'Class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Model Training & Selection\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "- **Metric**: Recall (prioritize catching churners)\n",
    "- **Why Recall?**: Missing a churner ($1,500 loss) is 15\u00d7 more costly than false alarm ($100 retention cost)\n",
    "- **Models**: Logistic Regression, Random Forest, XGBoost, LightGBM\n",
    "- **Validation**: 5-fold stratified cross-validation\n",
    "- **Hyperparameters**: RandomizedSearchCV (20 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "}\n",
    "\n",
    "# Hyperparameter grids (simplified for Kaggle runtime)\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l2']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "print('Models defined:')\n",
    "for name in models.keys():\n",
    "    print(f'  \u2022 {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with hyperparameter tuning\n",
    "print('Training models with hyperparameter optimization...')\n",
    "print('='*60)\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nTraining {name}...')\n",
    "    \n",
    "    # Randomized search\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_grids[name],\n",
    "        n_iter=10,  # Reduced for Kaggle runtime\n",
    "        scoring='recall',\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Store best model\n",
    "    best_models[name] = search.best_estimator_\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = search.best_estimator_.predict(X_test_scaled)\n",
    "    y_pred_proba = search.best_estimator_.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'CV Score': search.best_score_\n",
    "    }\n",
    "    \n",
    "    print(f'  \u2713 Best CV Recall: {search.best_score_:.4f}')\n",
    "    print(f'  \u2713 Test Recall: {results[name][\"Recall\"]:.4f}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('\u2713 All models trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('Recall', ascending=False)\n",
    "\n",
    "print('\\nModel Performance Comparison:')\n",
    "print('='*60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Best model\n",
    "best_model_name = results_df.index[0]\n",
    "print(f'\\n\ud83c\udfc6 Best Model: {best_model_name}')\n",
    "print(f'   Recall: {results_df.loc[best_model_name, \"Recall\"]:.1%}')\n",
    "print(f'   Accuracy: {results_df.loc[best_model_name, \"Accuracy\"]:.1%}')\n",
    "print(f'   ROC AUC: {results_df.loc[best_model_name, \"ROC AUC\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar plot of key metrics\n",
    "results_df[['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']].plot(\n",
    "    kind='bar', ax=axes[0], rot=45\n",
    ")\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].axhline(y=0.9, color='r', linestyle='--', alpha=0.3, label='90% threshold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Recall comparison (primary metric)\n",
    "results_df['Recall'].plot(kind='barh', ax=axes[1], color='#3498db')\n",
    "axes[1].set_title('Recall Comparison (Primary Metric)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Recall Score')\n",
    "axes[1].set_ylabel('Model')\n",
    "axes[1].axvline(x=0.9, color='r', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%.3f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Detailed Model Evaluation\n",
    "\n",
    "Analyzing the best model (XGBoost) in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_model = best_models[best_model_name]\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print('Confusion Matrix Analysis:')\n",
    "print('='*60)\n",
    "print(f'True Negatives (TN):  {tn:>4} | Correctly predicted no churn')\n",
    "print(f'False Positives (FP): {fp:>4} | Incorrectly predicted churn')\n",
    "print(f'False Negatives (FN): {fn:>4} | Missed churners (COSTLY!)')\n",
    "print(f'True Positives (TP):  {tp:>4} | Correctly predicted churn')\n",
    "print('\\n' + '='*60)\n",
    "print(f'Total Test Samples:   {len(y_test):>4}')\n",
    "print(f'Actual Churners:      {(fn + tp):>4}')\n",
    "print(f'Predicted Churners:   {(fp + tp):>4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix and metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0],\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "axes[1].plot(fpr, tpr, color='#3498db', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate (Recall)')\n",
    "axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "axes[2].plot(recall, precision, color='#e74c3c', lw=2, label='PR Curve')\n",
    "axes[2].axhline(y=y_test.mean(), color='gray', linestyle='--', label=f'Baseline ({y_test.mean():.2f})')\n",
    "axes[2].set_xlim([0.0, 1.0])\n",
    "axes[2].set_ylim([0.0, 1.05])\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(loc='upper right')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. SHAP Explainability Analysis\n",
    "\n",
    "Using SHAP (SHapley Additive exPlanations) to understand model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print('Creating SHAP explainer...')\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values (sample for speed)\n",
    "shap_sample_size = 100\n",
    "X_test_sample = X_test_scaled.sample(n=shap_sample_size, random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Handle multi-output (get churn class)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "print(f'\u2713 SHAP values calculated for {shap_sample_size} samples')\n",
    "print(f'Shape: {shap_values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (global feature importance)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, max_display=15, show=False)\n",
    "plt.title('SHAP Feature Importance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udcca SHAP Summary Plot shows:')\n",
    "print('  \u2022 Features ranked by importance (top to bottom)')\n",
    "print('  \u2022 Red = high feature value, Blue = low feature value')\n",
    "print('  \u2022 Right = increases churn probability, Left = decreases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (mean absolute SHAP values)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type='bar', max_display=15, show=False)\n",
    "plt.title('Top 15 Features by Mean |SHAP|', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from SHAP\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_test_sample.columns,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('\\nTop 10 Most Important Features:')\n",
    "print('='*60)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"{row['feature']:30s} | {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot for single prediction\n",
    "sample_idx = 0\n",
    "sample_customer = X_test_sample.iloc[sample_idx]\n",
    "sample_shap = shap_values[sample_idx]\n",
    "\n",
    "# Create explanation object\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, (list, np.ndarray)):\n",
    "    expected_value = expected_value[1]\n",
    "\n",
    "explanation = shap.Explanation(\n",
    "    values=sample_shap,\n",
    "    base_values=expected_value,\n",
    "    data=sample_customer.values,\n",
    "    feature_names=sample_customer.index.tolist()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.plots.waterfall(explanation, max_display=15, show=False)\n",
    "plt.title('SHAP Waterfall: Individual Prediction Explanation', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udca7 Waterfall Plot shows:')\n",
    "print('  \u2022 How each feature contributes to this specific prediction')\n",
    "print('  \u2022 Red = pushes prediction toward churn')\n",
    "print('  \u2022 Blue = pushes prediction away from churn')\n",
    "print(f'  \u2022 Base value (E[f(x)]): {expected_value:.3f}')\n",
    "print(f'  \u2022 Final prediction (f(x)): {expected_value + sample_shap.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from SHAP Analysis\n",
    "\n",
    "1. **Contract Type**: Two-year contracts strongly reduce churn risk\n",
    "2. **Tenure**: Longer tenure significantly reduces churn probability\n",
    "3. **Internet Service**: Fiber optic without premium services increases risk\n",
    "4. **Payment Method**: Electronic check increases churn risk\n",
    "5. **Tech Support**: Lack of tech support increases churn probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Business Impact Analysis\n",
    "\n",
    "### ROI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business parameters\n",
    "CUSTOMER_LIFETIME_VALUE = 2000  # $\n",
    "RETENTION_COST = 100  # $ per campaign\n",
    "SUCCESS_RATE = 0.65  # 65% of campaigns succeed\n",
    "\n",
    "# Calculate business metrics\n",
    "print('Business Impact Analysis')\n",
    "print('='*60)\n",
    "\n",
    "# Campaigns (all positive predictions)\n",
    "total_campaigns = tp + fp\n",
    "campaign_cost = total_campaigns * RETENTION_COST\n",
    "\n",
    "# Customers saved (TP \u00d7 success rate)\n",
    "customers_saved = int(tp * SUCCESS_RATE)\n",
    "revenue_saved = customers_saved * CUSTOMER_LIFETIME_VALUE\n",
    "\n",
    "# Customers lost (FN)\n",
    "customers_lost = fn\n",
    "revenue_lost = customers_lost * CUSTOMER_LIFETIME_VALUE\n",
    "\n",
    "# Net benefit\n",
    "net_benefit = revenue_saved - campaign_cost\n",
    "roi_percentage = (net_benefit / campaign_cost) * 100\n",
    "\n",
    "print(f'\\nRetention Campaign Metrics:')\n",
    "print(f'  Campaigns Run:           {total_campaigns:>6,} ({tp} TP + {fp} FP)')\n",
    "print(f'  Campaign Cost:           ${campaign_cost:>6,}')\n",
    "print(f'\\nOutcomes:')\n",
    "print(f'  Customers Saved:         {customers_saved:>6,} ({tp} \u00d7 {SUCCESS_RATE:.0%})')\n",
    "print(f'  Revenue Saved:           ${revenue_saved:>6,}')\n",
    "print(f'  Customers Lost:          {customers_lost:>6,} (missed churners)')\n",
    "print(f'  Revenue Lost:            ${revenue_lost:>6,}')\n",
    "print(f'\\nFinancial Impact:')\n",
    "print(f'  Net Benefit:             ${net_benefit:>6,}')\n",
    "print(f'  ROI:                     {roi_percentage:>6.1f}%')\n",
    "print(f'\\n\ud83d\udcb0 For every $1 spent, we gain ${net_benefit/campaign_cost:.2f} in net value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize business impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Cost vs Revenue\n",
    "categories = ['Campaign Cost', 'Revenue Saved', 'Net Benefit']\n",
    "values = [campaign_cost, revenue_saved, net_benefit]\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "bars = axes[0].bar(categories, values, color=colors, alpha=0.7)\n",
    "axes[0].set_title('Financial Impact', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Amount ($)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'${int(height):,}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Customers Saved vs Lost\n",
    "customer_data = {\n",
    "    'Saved': customers_saved,\n",
    "    'Lost': customers_lost,\n",
    "    'False Alarms': fp\n",
    "}\n",
    "colors2 = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "bars2 = axes[1].bar(customer_data.keys(), customer_data.values(), color=colors2, alpha=0.7)\n",
    "axes[1].set_title('Customer Outcomes', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Customers')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI Sensitivity Analysis\n",
    "print('\\nROI Sensitivity Analysis')\n",
    "print('='*60)\n",
    "\n",
    "# Test different scenarios\n",
    "scenarios = [\n",
    "    ('Pessimistic', 1500, 0.50),\n",
    "    ('Base Case', 2000, 0.65),\n",
    "    ('Optimistic', 2500, 0.75)\n",
    "]\n",
    "\n",
    "sensitivity_results = []\n",
    "\n",
    "for scenario_name, clv, success in scenarios:\n",
    "    saved = int(tp * success)\n",
    "    revenue = saved * clv\n",
    "    cost = total_campaigns * RETENTION_COST\n",
    "    net = revenue - cost\n",
    "    roi = (net / cost) * 100\n",
    "    \n",
    "    sensitivity_results.append({\n",
    "        'Scenario': scenario_name,\n",
    "        'CLV': f'${clv}',\n",
    "        'Success Rate': f'{success:.0%}',\n",
    "        'Customers Saved': saved,\n",
    "        'Net Benefit': f'${net:,}',\n",
    "        'ROI': f'{roi:.1f}%'\n",
    "    })\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_results)\n",
    "print(sensitivity_df.to_string(index=False))\n",
    "\n",
    "print('\\n\u2705 Business case remains strong across all scenarios')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### Model Performance\n",
    "- **Best Model**: XGBoost with 93% recall\n",
    "- **Optimization**: Focused on recall to minimize missed churners (costly false negatives)\n",
    "- **Validation**: Strong performance across statistical tests and confidence intervals\n",
    "\n",
    "### Business Impact\n",
    "- **Annual Savings**: $367,300 net benefit\n",
    "- **ROI**: 431.6% (every $1 spent returns $5.32)\n",
    "- **Customers Saved**: 226 annually (at 65% success rate)\n",
    "- **Robust**: Positive ROI even in pessimistic scenarios\n",
    "\n",
    "### Key Churn Drivers (from SHAP)\n",
    "1. **Contract Type** - Month-to-month has 42% churn\n",
    "2. **Tenure** - New customers (<12 months) at highest risk\n",
    "3. **Payment Method** - Electronic check users show 45% churn\n",
    "4. **Tech Support** - Lack of support increases churn 35%\n",
    "5. **Internet Service** - Fiber optic without premium services\n",
    "\n",
    "### Actionable Recommendations\n",
    "1. **Early Engagement** - Focus retention on customers <12 months tenure\n",
    "2. **Contract Upgrades** - Incentivize annual/2-year contracts\n",
    "3. **Service Bundling** - Promote tech support + security packages\n",
    "4. **Payment Migration** - Move electronic check users to automatic payments\n",
    "5. **Pricing Strategy** - Review high monthly charge customers for loyalty discounts\n",
    "\n",
    "### Technical Achievements\n",
    "- End-to-end ML pipeline with feature engineering\n",
    "- SMOTE for class imbalance handling\n",
    "- Hyperparameter optimization via RandomizedSearchCV\n",
    "- SHAP explainability for model interpretability\n",
    "- Comprehensive evaluation with business metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Deploy model** as REST API for real-time scoring\n",
    "2. **A/B test** retention campaigns on predicted high-risk customers\n",
    "3. **Monitor performance** and retrain quarterly\n",
    "4. **Expand features** with customer service notes (NLP), usage patterns\n",
    "5. **Segment strategies** - different approaches for tenure groups\n",
    "\n",
    "---\n",
    "\n",
    "**Full Project**: [GitHub Repository](https://github.com/noahgallagher1/customer-churn-prediction)\n",
    "\n",
    "**Author**: Noah Gallagher | [LinkedIn](https://www.linkedin.com/in/noahgallagher/) | [Portfolio](https://noahgallagher1.github.io/MySite/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}