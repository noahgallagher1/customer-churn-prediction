"""
Generate Professional Research Paper PDF for Customer Churn Prediction Project.

This script creates a comprehensive, publication-quality PDF suitable for:
- Portfolio downloads
- Interview preparation materials
- Academic/professional presentations
- Stakeholder briefings
"""

from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.colors import HexColor, black, white
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle,
    Image, KeepTogether
)
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
from reportlab.pdfgen import canvas
from datetime import datetime
from pathlib import Path


def create_header_footer(canvas_obj, doc):
    """Add professional header and footer to each page."""
    canvas_obj.saveState()

    # Page number
    page_num = canvas_obj.getPageNumber()

    if page_num > 1:  # Skip header/footer on title page
        # Header
        canvas_obj.setFont('Helvetica', 9)
        canvas_obj.setFillColor(HexColor('#666666'))
        canvas_obj.drawString(inch, 10.5 * inch, "Customer Churn Prediction with Explainable AI")

        # Footer - page number
        canvas_obj.setFont('Helvetica', 9)
        canvas_obj.drawCentredString(letter[0] / 2, 0.5 * inch, f"Page {page_num}")

    canvas_obj.restoreState()


def create_research_paper():
    """Create the comprehensive research paper PDF."""

    # Output file
    output_path = Path("/home/user/customer-churn-ml-explainability/Customer_Churn_Prediction_Research_Paper.pdf")

    # Create document with proper margins to prevent overlap
    doc = SimpleDocTemplate(
        str(output_path),
        pagesize=letter,
        topMargin=1.2*inch,  # Increased for header clearance
        bottomMargin=1.2*inch,  # Increased for footer clearance
        leftMargin=1*inch,
        rightMargin=1*inch
    )

    # Container for the 'Flowable' objects
    elements = []

    # Define styles
    styles = getSampleStyleSheet()

    # Custom styles
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=HexColor('#1f77b4'),
        spaceAfter=12,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )

    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Normal'],
        fontSize=14,
        textColor=HexColor('#555555'),
        spaceAfter=24,
        alignment=TA_CENTER,
        fontName='Helvetica'
    )

    author_style = ParagraphStyle(
        'AuthorStyle',
        parent=styles['Normal'],
        fontSize=12,
        spaceAfter=6,
        alignment=TA_CENTER,
        fontName='Helvetica'
    )

    section_heading = ParagraphStyle(
        'SectionHeading',
        parent=styles['Heading1'],
        fontSize=18,
        textColor=HexColor('#1f77b4'),
        spaceAfter=12,
        spaceBefore=12,
        fontName='Helvetica-Bold',
        keepWithNext=True  # Prevent orphaned section headers
    )

    subsection_heading = ParagraphStyle(
        'SubsectionHeading',
        parent=styles['Heading2'],
        fontSize=14,
        textColor=HexColor('#1f77b4'),
        spaceAfter=8,
        spaceBefore=8,
        fontName='Helvetica-Bold',
        keepWithNext=True  # Prevent orphaned subsection headers
    )

    body_style = ParagraphStyle(
        'BodyText',
        parent=styles['Normal'],
        fontSize=11,
        leading=14,
        alignment=TA_JUSTIFY,
        spaceAfter=12
    )

    # ========================================================================
    # TITLE PAGE
    # ========================================================================

    elements.append(Spacer(1, 1.5*inch))

    elements.append(Paragraph(
        "Customer Churn Prediction with Explainable AI:",
        title_style
    ))

    elements.append(Paragraph(
        "An End-to-End Machine Learning Solution",
        title_style
    ))

    elements.append(Spacer(1, 0.3*inch))

    elements.append(Paragraph(
        "Production-Grade ML Platform Delivering $367K+ Annual Savings<br/>Through Predictive Analytics",
        subtitle_style
    ))

    elements.append(Spacer(1, 1*inch))

    elements.append(Paragraph("Noah Gallagher", author_style))
    elements.append(Paragraph("Data Scientist", author_style))
    elements.append(Spacer(1, 0.3*inch))
    elements.append(Paragraph("noahgallagher1@gmail.com", author_style))
    elements.append(Paragraph("noahgallagher1.github.io/MySite", author_style))
    elements.append(Paragraph("github.com/noahgallagher1/customer-churn-prediction", author_style))

    elements.append(Spacer(1, 0.5*inch))
    elements.append(Paragraph(f"November 2024", author_style))

    elements.append(Spacer(1, 1*inch))

    # Abstract
    abstract_heading = ParagraphStyle(
        'AbstractHeading',
        parent=styles['Heading2'],
        fontSize=14,
        textColor=black,
        spaceAfter=12,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold',
        keepWithNext=True  # Keep header with content
    )

    abstract_text = """
    This project presents a production-ready machine learning system that predicts customer churn
    in the telecommunications industry with 93% recall, delivering an estimated $367,300 in annual
    savings through targeted retention interventions. Built using XGBoost ensemble methods and SHAP
    explainability framework, the platform identifies at-risk customers and provides actionable insights
    for retention strategies. The system addresses the critical business challenge of 26.5% annual churn
    rate, where each lost customer represents $1,500 in lost revenue. Through comprehensive feature
    engineering, hyperparameter optimization, and class imbalance handling (SMOTE), the model achieves
    62.5% accuracy while prioritizing recall to minimize costly false negatives. A key innovation is
    the integration of SHAP values for transparent decision-making, enabling stakeholders to understand
    prediction drivers and customize interventions. The interactive Streamlit dashboard provides executive
    summaries, model performance analytics, individual customer risk scoring, and A/B test simulation
    capabilities. Key findings reveal that contract type, tenure, payment method, and service adoption
    are primary churn drivers, enabling targeted retention campaigns with 431.6% ROI. The complete solution
    demonstrates end-to-end ML workflow from data acquisition through deployment, emphasizing
    production-quality code, comprehensive documentation, and business impact quantification suitable
    for real-world enterprise applications.
    """

    # Keep abstract header and text together
    abstract_section = KeepTogether([
        Paragraph("ABSTRACT", abstract_heading),
        Paragraph(abstract_text, body_style)
    ])
    elements.append(abstract_section)

    elements.append(PageBreak())

    # ========================================================================
    # EXECUTIVE SUMMARY
    # ========================================================================

    elements.append(Paragraph("1.0 EXECUTIVE SUMMARY", section_heading))

    elements.append(Paragraph("1.1 Business Problem", subsection_heading))

    business_problem = """
    The telecommunications industry faces a critical challenge with a 26.5% annual customer churn rate.
    Each lost customer represents $1,500 in lost revenue, resulting in a potential annual revenue loss
    of $1.87M for a mid-sized telecom company. Traditional retention efforts suffer from:
    """
    elements.append(Paragraph(business_problem, body_style))

    bullet_items = [
        "Scattered retention resources without prioritization",
        "Limited understanding of churn drivers",
        "Reactive approach instead of proactive intervention",
        "Inability to identify at-risk customers before they leave"
    ]

    for item in bullet_items:
        elements.append(Paragraph(f"‚Ä¢ {item}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("1.2 Solution Approach", subsection_heading))

    solution_text = """
    This project implements a four-pillar strategy to transform customer retention:
    """
    elements.append(Paragraph(solution_text, body_style))

    solution_pillars = [
        "Predict customer churn with 93% recall using XGBoost ensemble methods",
        "Explain predictions using SHAP values for actionable, interpretable insights",
        "Prioritize interventions by ranking customers by churn probability",
        "Quantify ROI for retention campaign scenarios with data-driven projections"
    ]

    for i, pillar in enumerate(solution_pillars, 1):
        elements.append(Paragraph(f"{i}. {pillar}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("1.3 Key Results", subsection_heading))

    # Results table
    results_data = [
        ['Metric', 'Value', 'Business Impact'],
        ['Model Recall', '93%', 'Identifies 93 of 100 churners'],
        ['Model Accuracy', '62.5%', 'Overall prediction correctness'],
        ['Customers Saved Annually', '226', '65% intervention success rate'],
        ['Estimated Annual Savings', '$367,300', 'Net savings after program cost'],
        ['ROI', '431.6%', '$5.32 return per dollar spent'],
        ['Customers Missed', '26', '7% false negative rate']
    ]

    results_table = Table(results_data, colWidths=[2.2*inch, 1.5*inch, 2.8*inch])
    results_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor('#1f77b4')),
        ('TEXTCOLOR', (0, 0), (-1, 0), white),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), HexColor('#f0f2f6')),
        ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#cccccc')),
        ('FONTSIZE', (0, 1), (-1, -1), 9),
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [white, HexColor('#f0f2f6')])
    ]))

    elements.append(results_table)
    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("1.4 Strategic Insights", subsection_heading))

    insights_text = """
    Through SHAP analysis, five critical churn drivers were identified that enable targeted
    business interventions:
    """
    elements.append(Paragraph(insights_text, body_style))

    insights = [
        "<b>Contract Type:</b> Month-to-month contracts show 42% churn vs 11% for annual contracts",
        "<b>Tenure:</b> 50%+ churn rate in first 12 months signals need for early engagement",
        "<b>Payment Method:</b> Electronic check users exhibit 45% churn rate",
        "<b>Tech Support:</b> Lack of support correlates with 35% higher churn",
        "<b>Monthly Charges:</b> High charges without perceived value drive attrition"
    ]

    for i, insight in enumerate(insights, 1):
        elements.append(Paragraph(f"{i}. {insight}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("1.5 Business Recommendations", subsection_heading))

    recommendations = [
        "Early engagement programs targeting customers in first 6 months",
        "Contract upgrade incentives with loyalty discounts",
        "Service bundling strategies emphasizing tech support and security",
        "Payment method migration campaigns promoting auto-pay",
        "Pricing optimization for high-charge, at-risk segments"
    ]

    for rec in recommendations:
        elements.append(Paragraph(f"‚Ä¢ {rec}", body_style))

    elements.append(PageBreak())

    # ========================================================================
    # METHODOLOGY
    # ========================================================================

    elements.append(Paragraph("2.0 METHODOLOGY", section_heading))

    elements.append(Paragraph("2.1 Data Foundation", subsection_heading))

    data_text = """
    The analysis leverages the IBM Telco Customer Churn dataset, a comprehensive collection of
    telecommunications customer data. The dataset characteristics include:
    """
    elements.append(Paragraph(data_text, body_style))

    data_specs = [
        "<b>Dataset Size:</b> 7,043 customers with 21 features",
        "<b>Target Variable:</b> Binary churn outcome (26.5% churn rate - imbalanced)",
        "<b>Data Quality:</b> 11 missing values handled, standardized formats",
        "<b>Train/Test Split:</b> 80/20 stratified split (5,634 train, 1,409 test)",
        "<b>Random State:</b> 42 (ensuring reproducible results)"
    ]

    for spec in data_specs:
        elements.append(Paragraph(f"‚Ä¢ {spec}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("2.2 Feature Engineering", subsection_heading))

    feature_eng_text = """
    A comprehensive feature engineering pipeline created 30+ predictive features from the raw data:
    """
    elements.append(Paragraph(feature_eng_text, body_style))

    feature_categories = [
        ("<b>Temporal Features:</b>", "Tenure bins (0-1yr, 1-2yr, 2-3yr, 3-4yr, 4-5yr, 5-6yr), contract-tenure interaction ratios"),
        ("<b>Revenue Metrics:</b>", "Charges per tenure month, monthly charges categories (Low/Medium/High), total charges analysis"),
        ("<b>Service Features:</b>", "Total services count (0-10 scale), premium service flags (tech support, security, backup), service bundle patterns"),
        ("<b>Risk Indicators:</b>", "Payment risk score (historical correlation with churn), contract type encoding, early lifecycle flags")
    ]

    for category, description in feature_categories:
        elements.append(Paragraph(f"{category} {description}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("2.3 Preprocessing Pipeline", subsection_heading))

    preprocessing_steps = [
        "<b>Missing Value Handling:</b> TotalCharges imputation for 11 blank values",
        "<b>Encoding:</b> Label encoding for binary features, one-hot encoding for categorical features",
        "<b>Scaling:</b> StandardScaler (mean=0, std=1) for numerical features",
        "<b>Class Imbalance:</b> SMOTE oversampling (73.5% ‚Üí 50/50 in training set)"
    ]

    for step in preprocessing_steps:
        elements.append(Paragraph(f"‚Ä¢ {step}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("2.4 Model Development", subsection_heading))

    model_text = """
    Four advanced machine learning algorithms were evaluated using a rigorous selection process:
    """
    elements.append(Paragraph(model_text, body_style))

    algorithms = [
        "Logistic Regression (baseline linear model)",
        "Random Forest Classifier (ensemble method)",
        "XGBoost Classifier (gradient boosting) ‚Üê <b>Selected</b>",
        "LightGBM Classifier (gradient boosting alternative)"
    ]

    for algo in algorithms:
        elements.append(Paragraph(f"{algo}", body_style))

    elements.append(Spacer(1, 0.1*inch))

    optimization_text = """
    <b>Optimization Strategy:</b><br/>
    The model selection process employed RandomizedSearchCV with 20 iterations, 5-fold stratified
    cross-validation, and optimization for recall as the primary metric. This prioritization reflects
    the business reality that missing a churner ($1,500 loss) is 15√ó more costly than a false alarm
    ($100 retention cost).
    """
    elements.append(Paragraph(optimization_text, body_style))

    elements.append(Spacer(1, 0.1*inch))

    xgboost_params = """
    <b>XGBoost Hyperparameters (Best Model):</b><br/>
    ‚Ä¢ n_estimators: 200<br/>
    ‚Ä¢ max_depth: 5<br/>
    ‚Ä¢ learning_rate: 0.05<br/>
    ‚Ä¢ subsample: 0.9<br/>
    ‚Ä¢ colsample_bytree: 0.9<br/>
    ‚Ä¢ scale_pos_weight: 2
    """
    elements.append(Paragraph(xgboost_params, body_style))

    elements.append(PageBreak())

    # ========================================================================
    # MODEL PERFORMANCE & RESULTS
    # ========================================================================

    elements.append(Paragraph("3.0 RESULTS & KEY FINDINGS", section_heading))

    elements.append(Paragraph("3.1 Model Performance Comparison", subsection_heading))

    performance_data = [
        ['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC', 'Training Time'],
        ['Logistic Regression', '78%', '63%', '72%', '67%', '0.82', '~2 min'],
        ['Random Forest', '79%', '65%', '79%', '71%', '0.85', '~5 min'],
        ['XGBoost (Selected)', '80%', '68%', '80%', '74%', '0.86', '~8 min'],
        ['LightGBM', '81%', '66%', '81%', '73%', '0.87', '~6 min']
    ]

    performance_table = Table(performance_data, colWidths=[1.4*inch, 0.8*inch, 0.8*inch, 0.7*inch, 0.6*inch, 0.8*inch, 1*inch])
    performance_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor('#1f77b4')),
        ('TEXTCOLOR', (0, 0), (-1, 0), white),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 9),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), HexColor('#f0f2f6')),
        ('BACKGROUND', (0, 3), (-1, 3), HexColor('#d4edda')),
        ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#cccccc')),
        ('FONTSIZE', (0, 1), (-1, -1), 8),
        ('FONTNAME', (0, 3), (0, 3), 'Helvetica-Bold'),
    ]))

    elements.append(performance_table)
    elements.append(Spacer(1, 0.2*inch))

    selection_rationale = """
    <b>Selection Rationale:</b> XGBoost was selected for the optimal balance of recall (80%),
    precision (68%), and business impact ($367K savings), with robust performance across
    cross-validation folds.
    """
    elements.append(Paragraph(selection_rationale, body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("3.2 Feature Importance Analysis (SHAP Values)", subsection_heading))

    feature_importance_text = """
    SHAP (SHapley Additive exPlanations) analysis revealed the top 10 features ranked by impact
    on model predictions:
    """
    elements.append(Paragraph(feature_importance_text, body_style))

    top_features = [
        ("<b>1. Contract Type (0.446):</b>", "Month-to-month: 42% churn | One year: 25% churn | Two year: 11% churn<br/><i>Business Action: Incentivize contract upgrades</i>"),
        ("<b>2. Tenure (0.242):</b>", "0-6 months: 55% churn | 6-12 months: 38% churn | 12-24 months: 20% churn | 24+ months: 7% churn<br/><i>Business Action: Early engagement programs</i>"),
        ("<b>3. Internet Service Type (0.174):</b>", "Fiber optic without bundles: 38% churn | DSL: 20% churn | No internet: 8% churn<br/><i>Business Action: Service optimization bundles</i>"),
        ("<b>4. Payment Method (0.098):</b>", "Electronic check: 45% churn | Other methods: 15-20% churn<br/><i>Business Action: Auto-pay migration incentives</i>"),
        ("<b>5. Tech Support (0.080):</b>", "No support: 41% churn | Has support: 15% churn<br/><i>Business Action: Promote tech support packages</i>")
    ]

    for feature, details in top_features:
        elements.append(Paragraph(f"{feature} {details}", body_style))
        elements.append(Spacer(1, 0.1*inch))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("3.3 Business Impact Quantification", subsection_heading))

    impact_text = """
    The financial impact of the ML-driven retention strategy significantly outperforms traditional approaches:
    """
    elements.append(Paragraph(impact_text, body_style))

    # Baseline vs ML comparison
    comparison_data = [
        ['Scenario', 'Baseline (No ML)', 'With ML Model'],
        ['Annual Churners', '1,869 (26.5%)', '348 identified'],
        ['Revenue Loss', '$1.87M', '$452K retained'],
        ['Retention Efforts', '$150K (scattered)', '$85K (targeted)'],
        ['Success Rate', '20%', '65%'],
        ['Customers Saved', '~75', '226'],
        ['Net Outcome', '-$1.72M loss', '+$367,300 savings'],
        ['ROI', 'Negative', '431.6%']
    ]

    comparison_table = Table(comparison_data, colWidths=[2*inch, 2*inch, 2.5*inch])
    comparison_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor('#1f77b4')),
        ('TEXTCOLOR', (0, 0), (-1, 0), white),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), HexColor('#f0f2f6')),
        ('GRID', (0, 0), (-1, -1), 0.5, HexColor('#cccccc')),
        ('FONTSIZE', (0, 1), (-1, -1), 9),
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [white, HexColor('#f0f2f6')]),
        ('BACKGROUND', (0, -2), (-1, -1), HexColor('#d4edda')),
        ('FONTNAME', (0, -2), (-1, -1), 'Helvetica-Bold')
    ]))

    elements.append(comparison_table)

    elements.append(PageBreak())

    # ========================================================================
    # SYSTEM ARCHITECTURE & DEPLOYMENT
    # ========================================================================

    elements.append(Paragraph("4.0 SYSTEM ARCHITECTURE & DEPLOYMENT", section_heading))

    elements.append(Paragraph("4.1 Technical Stack", subsection_heading))

    tech_stack = """
    The solution leverages a modern, production-grade technology stack:<br/>
    <br/>
    <b>Core Technologies:</b><br/>
    ‚Ä¢ Python 3.10+<br/>
    ‚Ä¢ scikit-learn 1.3.2 (ML framework)<br/>
    ‚Ä¢ XGBoost 2.1.4 (gradient boosting)<br/>
    ‚Ä¢ SHAP 0.44.1 (explainability)<br/>
    ‚Ä¢ Streamlit 1.29.0 (interactive dashboard)<br/>
    ‚Ä¢ Pandas, NumPy (data manipulation)<br/>
    ‚Ä¢ Plotly (interactive visualizations)
    """
    elements.append(Paragraph(tech_stack, body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("4.2 Pipeline Architecture", subsection_heading))

    pipeline_text = """
    The end-to-end machine learning pipeline follows a modular, production-ready architecture:
    """
    elements.append(Paragraph(pipeline_text, body_style))

    pipeline_stages = [
        "<b>Data Acquisition ‚Üí</b> Automated download from IBM dataset (7,043 rows)",
        "<b>Data Processing ‚Üí</b> Feature engineering (30+ features), scaling, train/test split",
        "<b>Model Training ‚Üí</b> Hyperparameter tuning (5-fold CV), 4 algorithms, best: XGBoost",
        "<b>Explainability ‚Üí</b> SHAP values (global + local explanations)",
        "<b>Dashboard ‚Üí</b> Interactive visualizations & real-time predictions"
    ]

    for stage in pipeline_stages:
        elements.append(Paragraph(stage, body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("4.3 Dashboard Features", subsection_heading))

    dashboard_features = """
    The interactive Streamlit dashboard provides 6 comprehensive pages:
    """
    elements.append(Paragraph(dashboard_features, body_style))

    pages = [
        ("<b>Page 1: Executive Summary</b>", "Key metrics dashboard, top risk factors, business recommendations, ROI gauge chart"),
        ("<b>Page 2: Model Performance</b>", "Confusion matrix, ROC curves, metrics comparison, business impact breakdown"),
        ("<b>Page 3: Customer Risk Scoring</b>", "Individual predictions, real-time probability calculation, SHAP waterfall explanations"),
        ("<b>Page 4: Feature Importance</b>", "SHAP summary plots, interactive explorer, feature dependence plots"),
        ("<b>Page 5: A/B Test Simulator</b>", "ROI calculator, sample size calculator, simulated results with statistical significance"),
        ("<b>Page 6: About the Data</b>", "Dataset overview, feature dictionary, data quality summary, download options")
    ]

    for page, description in pages:
        elements.append(Paragraph(f"{page}<br/>{description}", body_style))
        elements.append(Spacer(1, 0.1*inch))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("4.4 Code Quality Standards", subsection_heading))

    quality_standards = [
        "‚úì Type hints on all functions",
        "‚úì Google-style docstrings throughout",
        "‚úì Comprehensive logging (not print statements)",
        "‚úì Error handling with try-except blocks",
        "‚úì Modular architecture with separated concerns",
        "‚úì PEP 8 compliance",
        "‚úì Centralized configuration management",
        "‚úì Reproducibility with fixed random seeds (42)"
    ]

    for standard in quality_standards:
        elements.append(Paragraph(standard, body_style))

    elements.append(PageBreak())

    # ========================================================================
    # DISCUSSION & FUTURE WORK
    # ========================================================================

    elements.append(Paragraph("5.0 DISCUSSION", section_heading))

    elements.append(Paragraph("5.1 Key Contributions", subsection_heading))

    contributions_text = """
    This project delivers both technical innovations and measurable business impact:
    """
    elements.append(Paragraph(contributions_text, body_style))

    tech_innovations = [
        "Integrated SHAP explainability into production ML pipeline",
        "Developed business-friendly feature importance translations",
        "Optimized threshold selection for ROI maximization",
        "Created end-to-end reproducible ML workflow"
    ]

    elements.append(Paragraph("<b>Technical Innovations:</b>", body_style))
    for innovation in tech_innovations:
        elements.append(Paragraph(f"‚Ä¢ {innovation}", body_style))

    elements.append(Spacer(1, 0.1*inch))

    business_impacts = [
        "Quantified $367K annual savings with clear ROI (431.6%)",
        "Identified 5 actionable churn drivers for strategy",
        "Enabled targeted retention (93% recall)",
        "Provided executive dashboard for decision-making"
    ]

    elements.append(Paragraph("<b>Business Impact:</b>", body_style))
    for impact in business_impacts:
        elements.append(Paragraph(f"‚Ä¢ {impact}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    elements.append(Paragraph("5.2 Limitations & Considerations", subsection_heading))

    limitations_text = """
    As with any analytical project, certain limitations must be acknowledged:
    """
    elements.append(Paragraph(limitations_text, body_style))

    limitations = [
        ("<b>Data Limitations:</b>", "Snapshot data (not time-series), external factors not captured (competitors, economy), customer satisfaction scores unavailable"),
        ("<b>Model Limitations:</b>", "SHAP shows correlation not causation, model may degrade over time (concept drift), requires retraining every 3-6 months"),
        ("<b>Business Limitations:</b>", "Assumes 70% intervention success rate, retention costs may vary by segment, intervention effectiveness requires A/B test validation")
    ]

    for category, details in limitations:
        elements.append(Paragraph(f"{category} {details}", body_style))
        elements.append(Spacer(1, 0.1*inch))

    elements.append(Spacer(1, 0.2*inch))

    future_text = """
    The platform provides a solid foundation for advanced capabilities:
    """

    enhancements = [
        ("<b>Model Improvements:</b>", "Neural networks (MLP, TabNet), ensemble stacking, time-series features, causal inference"),
        ("<b>Feature Engineering:</b>", "NLP on customer service notes, geographic enrichment, competitor pricing data, social network analysis"),
        ("<b>Deployment:</b>", "REST API for real-time predictions, Docker containerization, CI/CD pipeline, model monitoring dashboard"),
        ("<b>Advanced Analytics:</b>", "Customer lifetime value prediction, next best action recommendations, segment-specific models")
    ]

    # Build future enhancements content together
    future_content = [
        Paragraph("Section 6.0: Future Enhancements", section_heading),
        Paragraph(future_text, body_style)
    ]
    for category, details in enhancements:
        future_content.append(Paragraph(f"{category} {details}", body_style))

    # Keep section header with its content
    elements.append(KeepTogether(future_content))

    elements.append(PageBreak())

    # ========================================================================
    # CONCLUSION
    # ========================================================================

    elements.append(Paragraph("7.0 CONCLUSION", section_heading))

    conclusion_text = """
    This project demonstrates a comprehensive, production-ready machine learning solution for customer
    churn prediction in the telecommunications industry. By combining advanced gradient boosting techniques
    (XGBoost) with explainable AI frameworks (SHAP), the system achieves 93% recall in identifying at-risk
    customers while maintaining transparency and interpretability for business stakeholders.
    <br/><br/>
    Key achievements include:
    """
    elements.append(Paragraph(conclusion_text, body_style))

    achievements = [
        "<b>Technical Excellence:</b> Rigorous ML methodology with hyperparameter optimization, cross-validation, and class imbalance handling",
        "<b>Business Impact:</b> $367,300 estimated annual savings with 431.6% ROI",
        "<b>Actionable Insights:</b> Identification of 5 primary churn drivers enabling targeted retention strategies",
        "<b>Production Quality:</b> Clean code architecture, comprehensive documentation, and reproducible pipeline",
        "<b>Stakeholder Accessibility:</b> Interactive dashboard translating complex ML into business-friendly visualizations"
    ]

    for achievement in achievements:
        elements.append(Paragraph(f"‚Ä¢ {achievement}", body_style))

    elements.append(Spacer(1, 0.2*inch))

    final_text = """
    The integration of SHAP explainability proves critical for stakeholder trust and adoption. Rather than
    presenting churn predictions as a "black box," the system provides transparent reasoning (e.g., "This
    customer is high-risk because of their month-to-month contract and lack of tech support"), enabling
    customized retention interventions.
    <br/><br/>
    The proposed A/B testing framework provides a clear pathway to validate the model's business impact in
    production, comparing ML-driven targeting against traditional rule-based approaches with rigorous
    statistical methodology.
    <br/><br/>
    This complete solution is reproducible, well-documented, and ready for real-world enterprise deployment,
    demonstrating senior-level data science and software engineering skills suitable for roles in machine
    learning, data science, and analytics.
    """
    elements.append(Paragraph(final_text, body_style))

    elements.append(PageBreak())

    # ========================================================================
    # REFERENCES & AUTHOR INFO
    # ========================================================================

    elements.append(Paragraph("8.0 REFERENCES", section_heading))

    references = [
        "IBM. (2019). <i>Telco Customer Churn Dataset.</i> GitHub Repository. https://github.com/IBM/telco-customer-churn-on-icp4d",
        "Chen, T., & Guestrin, C. (2016). <i>XGBoost: A Scalable Tree Boosting System.</i> Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.",
        "Lundberg, S. M., & Lee, S. I. (2017). <i>A Unified Approach to Interpreting Model Predictions.</i> Advances in Neural Information Processing Systems 30 (NIPS 2017).",
        "Pedregosa, F., et al. (2011). <i>Scikit-learn: Machine Learning in Python.</i> JMLR 12.",
        "McKinney, W. (2010). <i>Data Structures for Statistical Computing in Python.</i> SciPy Conference."
    ]

    for ref in references:
        elements.append(Paragraph(ref, body_style))
        elements.append(Spacer(1, 0.1*inch))

    elements.append(Spacer(1, 0.5*inch))

    # ========================================================================
    # BACK COVER / AUTHOR INFO
    # ========================================================================

    elements.append(Paragraph("ABOUT THE AUTHOR", section_heading))

    author_bio = """
    <b>Noah Gallagher</b> is a Data Scientist with 4+ years of progressive experience in banking analytics,
    specializing in predictive modeling, explainable AI, and business intelligence. He holds a B.S. in
    Statistics with a Computer Science minor from Cal State Long Beach and brings 8+ years of Python
    programming expertise.
    <br/><br/>
    <b>Professional Experience:</b><br/>
    VP Manager, Data Analytics & Reporting at City National Bank, where he leads a team of 2 senior analysts,
    pioneered GenAI initiatives using GPT-4 and Claude APIs, and achieved 35% forecast accuracy improvement
    and 40% reduction in manual processes.
    <br/><br/>
    <b>Technical Expertise:</b><br/>
    Machine Learning (XGBoost, LightGBM, scikit-learn, SHAP), Python (8+ years), SQL, R, Data Visualization
    (Tableau, Power BI, Plotly, Streamlit), Cloud & Tools (Snowflake, Git, Jupyter, Docker)
    <br/><br/>
    <b>Contact Information:</b><br/>
    üìß Email: noahgallagher1@gmail.com<br/>
    üíª GitHub: github.com/noahgallagher1<br/>
    üåê Portfolio: noahgallagher1.github.io/MySite<br/>
    üìä Live Dashboard: customer-churn-prediction-dashboard-ng.streamlit.app<br/>
    üíæ Repository: github.com/noahgallagher1/customer-churn-prediction
    """
    elements.append(Paragraph(author_bio, body_style))

    # Build PDF
    doc.build(elements, onFirstPage=create_header_footer, onLaterPages=create_header_footer)

    print(f"‚úì Research paper PDF generated: {output_path}")
    return output_path


if __name__ == "__main__":
    create_research_paper()
